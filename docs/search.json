[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Finding Outliers using Gaussian Mixture and DBSCAN\n\n\n\n\n\n\n\noutliers\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nMark Hamilton\n\n\n\n\n\n\n  \n\n\n\n\nClustering using K-means\n\n\n\n\n\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nMark Hamilton\n\n\n\n\n\n\n  \n\n\n\n\nRegression using Linear Regression and Random Forest\n\n\n\n\n\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nMark Hamilton\n\n\n\n\n\n\n  \n\n\n\n\nClassification using Logistic Regression and Random Forest\n\n\n\n\n\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nMark Hamilton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification using Logistic Regression and Random Forest",
    "section": "",
    "text": "Classification is a type of supervised machine learning that predicts the correct category of the given input data. To explore different aspects of classification, let’s try out some classification models on the breast cancer dataset from scikit-learn.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc\n\n\ncancer = load_breast_cancer()\nprint(f\"Number of entries: {cancer.target.size}\")\npd.DataFrame(cancer.data, columns=cancer.feature_names).head()\n\nNumber of entries: 569\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\nThe breast cancer dataset contains 569 data points, each with 30 numeric features of tumors, and the labels state whether the tumor is cancerous. Since there are only two unique labels (True/False), we will be doing binary classification.\nLet’s do a train test split with a test size of 20%.\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n\nFirst, let’s try logistic regression. Logistic regression determines the probability that a given data point is a part of a certain class.\n\nclf = LogisticRegression(max_iter=10000)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n\nAccuracy: 0.956140350877193\n\n\nWe trained the model on the training set, and used the test set to get the model’s predictions. We compared the predictions to the true labels, and we got 95.6% accuracy. That’s pretty good, but what if most of the data belonged to one class? So for this dataset, what if most of the tumors were not cancerous? Then if the model predicted false every time, the accuracy would be very good, but the model would be bad. So if there is a severe class imbalance, accuracy does not tell the whole story.\n\nnp.unique(cancer.target, return_counts=True)\n\n(array([0, 1]), array([212, 357], dtype=int64))\n\n\nThis class imbalance is not too severe, so accuracy is okay to use as a performance metric.\nBut let’s still discuss precision and recall. P&R are …\n\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n\nPrecision: 0.9459459459459459\nRecall: 0.9859154929577465\nF1 Score: 0.9655172413793103\n\n\nHere is the PR curve:\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n# Compute area under the curve (AUC)\nauc_score = auc(recall, precision)\n\n# Plot the Precision-Recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'PR Curve (AUC = {auc_score:.2f})', color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Logistic Regression')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\nHere is the ROC curve:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\n# plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='b')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve for Logistic Regression')\nplt.legend(loc='best')\nplt.show()\n\n\n\n\nNow let’s try Random Forest. RF is …\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9649122807017544\n\n\nPR curve:\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n\n# Compute area under the curve (AUC)\nauc_score = auc(recall, precision)\n\n# Plot the Precision-Recall curve\n# plt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label=f'PR Curve (AUC = {auc_score:.2f})', color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Random Forest')\nplt.legend(loc='best')\nplt.show()\n\nPrecision: 0.958904109589041\nRecall: 0.9859154929577465\nF1 Score: 0.9722222222222222\n\n\n\n\n\nROC curve:\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\n# plt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='b')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest')\nplt.legend(loc='best')\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification using Logistic Regression and Random Forest",
    "section": "",
    "text": "Classification is a type of supervised machine learning that predicts the correct category of each input data point. To explore different aspects of classification, let’s try out some classification models on the breast cancer dataset from scikit-learn.\n\n\nImports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(f\"Number of entries: {cancer.target.size}\")\nprint(f\"Unique Labels: {np.unique(cancer.target)}\")\npd.DataFrame(cancer.data, columns=cancer.feature_names).head()\n\nNumber of entries: 569\nUnique Labels: [0 1]\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\nThe breast cancer dataset contains 569 data points, each with 30 numeric features of a tumor, and the labels state whether the tumor is cancerous. Since there are only two unique labels (True/False), we will be doing binary classification.\nLet’s do a train test split with a test size of 20%.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n\nFirst, let’s try logistic regression. Logistic regression determines the probability that a given data point is a part of a certain class. Here, we are doing binary classification, so the model will choose the class that has a probability of over 50% as its prediction.\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=10000)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n\nAccuracy: 0.956140350877193\n\n\nWe trained the model on the training set, and used the test set to get the model’s predictions. We compared the predictions to the actual labels, and we got 95.6% accuracy. That’s pretty good, but what if most of the data belonged to one class? So for this dataset, what if most of the tumors were not cancerous? Then if the model predicted false every time, the accuracy would be very high, but the model would actually be terrible at identifying cancerous tumors. So if there is a severe class imbalance, accuracy does not tell the whole story.\n\nnp.unique(cancer.target, return_counts=True)\n\n(array([0, 1]), array([212, 357], dtype=int64))\n\n\nThis class imbalance is not too severe, so accuracy should be okay to use as a performance metric.\nBut let’s still discuss precision and recall, which are two performance metrics for classification that will work even when there is a class imbalance.\nPrecision is the percent of positive predictions made by the model that are actually positive.\n\\(Precision=\\frac{True Positives}{True Positives + False Positives}\\)\nRecall is the percent of positive data points that the model correctly predicted as positive.\n\\(Recall=\\frac{True Positives}{True Positives + False Negatives}\\)\nThere is a tradeoff between precision and recall. If the model predicts fewer positives, precision will improve because there will be fewer false positives, but recall will decrease because there may also be fewer true positives. If the model predicts more positives, precision will decrease and recall will increase.\nF1 score is a value that combines precision and recall. It is the harmonic mean of precision and recall.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n\nPrecision: 0.9459459459459459\nRecall: 0.9859154929577465\nF1 Score: 0.9655172413793103\n\n\nA Precision-Recall curve shows the tradeoff between precision and recall. A good model has both high precision and high recall, so it is better if the curve is closer to the top right. Here is the PR curve:\n\nfrom sklearn.metrics import precision_recall_curve, auc\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n# Compute area under the curve (AUC)\nauc_score = auc(recall, precision)\n\n# Plot the Precision-Recall curve\nplt.plot(recall, precision, label=f'PR Curve (AUC = {auc_score:.2f})', color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Logistic Regression')\nplt.legend()\nplt.show()\n\n\n\n\nThe area under the curve (AUC) represents how good the PR curve is. If the curve is closer to the top right, the AUC will be larger. But it is important to look at the curve in addition to the AUC, because you want a fairly even tradeoff between precision and recall.\nNow let’s try random forest, a different model that can be used for classification. Random forest consists of multiple decision trees. Each decision tree makes decisions based on a random subset of features from the dataset using bagging (random selection with replacement), and each decision tree votes to determine the overall model’s prediction.\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9649122807017544\n\n\nWe did not specify the number of decision trees to use, so it used 100 decision trees, which is the default value. The accuracy for this random forest model is better than that of our logistic regression model.\nHere are the performance metrics and the PR curve:\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\nprint(f\"Precision: {precision_score(y_test, y_pred)}\")\nprint(f\"Recall: {recall_score(y_test, y_pred)}\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n\n# Compute area under the curve (AUC)\nauc_score = auc(recall, precision)\n\n# Plot the Precision-Recall curve\nplt.plot(recall, precision, label=f'PR Curve (AUC = {auc_score:.2f})', color='b')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Random Forest')\nplt.legend()\nplt.show()\n\nPrecision: 0.958904109589041\nRecall: 0.9859154929577465\nF1 Score: 0.9722222222222222\n\n\n\n\n\nRandom forest also has better precision and the same recall, so it seems that random forest is a better model to use for this dataset. However, there are various hyperparameters for both of these models. I mostly used the default hyperparameters set by scikit-learn, but if I were to test different values for the hyperparameters, the performance of the models could improve."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Regression using Linear Regression and Random Forest",
    "section": "",
    "text": "Regression is a type of supervised machine learning that predicts the target value of each input data point. Regression is for predicting continuous values, while classification is for predicting discrete classes. Let’s use the diabetes dataset from scikit-learn to explore regression.\n\n\nImports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nfrom sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\nprint(f\"Shape of dataset: {diabetes.data.shape}\")\npd.DataFrame(diabetes.data, columns=diabetes.feature_names).head()\n\nShape of dataset: (442, 10)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n\n\n\n\n\n\n\nThe diabetes dataset contains measurements for 10 baseline variables from 442 diabetes patients. The label for each data point is a continuous numeric value that measures how much the diabetes progressed in one year.\nFirst, let’s do a train test split with a test size of 20%.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n\nLet’s see if we can accurately model the data using the linear regression model. Linear regression attempts find a linear relationship between the data and the labels by finding the best-fit line for the data. Since there are 10 input variables for each diabetes patient, it would be difficult to visualize the data and the best-fit line, because it would need to be in 10 dimensions. So first, let’s limit the model to only train on one input variable: age.\n\nfrom sklearn.linear_model import LinearRegression\nage = LinearRegression()\nage.fit(X_train[:, 0].reshape(-1, 1), y_train)\ny_pred = age.predict(X_test[:, 0].reshape(-1, 1))\n\n\nplt.scatter(X_train[:, 0], y_train, color='black', label='Actual data')\nplt.plot(X_test[:, 0], y_pred, color='blue', linewidth=3, label='Prediction')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Diabetes Progression\")\nplt.title(\"Linear Regression for Age\")\nplt.legend()\nplt.show()\nprint(\"Best fit line:\")\nprint(f\"y = {age.coef_[0]}x + {age.intercept_}\")\n\n\n\n\nBest fit line:\ny = 331.0721720041364x + 153.25920153319478\n\n\nIt looks like there is only a slight positive correlation between age and diabetes progression. Let’s calculate the \\(R^2\\) value for this prediction. \\(R^2\\) is a value between 0 and 1 that represents how well the dependent variable can be predicted by the independent variables. The plot showed only a slight correlation, so we should not expect the \\(R^2\\) value to be very high.\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.008289640305771506\n\n\nLet’s now try another one of the 10 independent variables: BMI.\n\nbmi = LinearRegression()\nbmi.fit(X_train[:, 2].reshape(-1, 1), y_train)\ny_pred = bmi.predict(X_test[:, 2].reshape(-1, 1))\n\n\nplt.scatter(X_train[:, 2], y_train, color='black', label='Actual data')\nplt.plot(X_test[:, 2], y_pred, color='blue', linewidth=3, label='Prediction')\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Diabetes Progression\")\nplt.title(\"Linear Regression for BMI\")\nplt.legend()\nplt.show()\nprint(f\"y = {bmi.coef_[0]}x + {bmi.intercept_}\")\n\n\n\n\ny = 998.577689137559x + 152.00335421448167\n\n\nThere is a positive correlation between BMI and diabetes progression, and the correlation is stronger than in the example with age and diabetes progression. We should expect a higher \\(R^2\\) value.\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.2333503981587215\n\n\nNow, let’s use all 10 input variables in our linear regression model.\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.4526027629719196\n\n\nThe \\(R^2\\) value when using all 10 input variables is even higher than when we only used BMI. This means the other 9 input variables combined helped the model predict more accurate values. But using more input variables would not necessarily improve the model’s performance, as some input variables may not be good predictors of diabetes progression.\nLet’s now try using random forest. We used random forest when doing classification, but it can also be used for regression. Instead of having each decision tree vote to determine the final prediction, for regression, the prediction is the average of the outputs from each decision tree. Random forest is a nonlinear model, so it should be more flexible when trying to understand more complex relationships in the data.\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.4428225673999313\n\n\nIn our example, the \\(R^2\\) value when using random forest is actually slightly worse than the \\(R^2\\) value when using linear regression! This could indicate that there is a fairly strong linear relationship in the data, and random forest is overthinking, but if that were the case, the \\(R^2\\) value for linear regression would be higher. The actual reason may be because there is no strong relationship when considering all 10 input variables while training, so both models struggle."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering using K-means",
    "section": "",
    "text": "Clustering is a type of unsupervised learning that attempts to group unlabeled data together. Let’s use the penguins dataset from Seaborn to explore clustering.\n\n\nImports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n# Suppress all warnings\nwarnings.simplefilter(\"ignore\")\n\n\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\npenguins.shape\n\n(344, 7)\n\n\n\nnp.unique(penguins[\"species\"]).tolist()\n\n['Adelie', 'Chinstrap', 'Gentoo']\n\n\nThe penguins dataset contains 344 data points describing 7 different features of penguins. There are 3 different penguin species: Adelie, Chinstrap, and Gentoo. As we can see, there are some null values in the table. Let’s get rid of those.\n\npenguins = penguins.dropna()\npenguins.shape\n\n(333, 7)\n\n\nWe dropped 11 rows with null values.\nLet’s perform k-means clustering on this dataset. K-means clustering is use to partition a dataset into \\(k\\) separate clusters. It works by first initializing \\(k\\) random points and treating them as centroids of a cluster. Each point determines its cluster by finding the nearest centroid. Then the centroids are updated to be at the middle of the cluster. Since the centroids move, each point now may be closer to a different centroid, so this process repeats until the centroids stop moving.\nLet’s use k-means clustering to group the dataset into clusters based on bill size. We will use bill length combined with bill depth to determine bill size. Each cluster will hopefully represent a different penguin species. Since we already know that there are 3 different penguin species, let’s set \\(k\\) equal to 3 for now.\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nkmeans_pred = pd.Series(kmeans.fit_predict(penguins[['bill_length_mm', 'bill_depth_mm']]))\n\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot for actual species\nspecies_colors = {'Adelie': 'red', 'Chinstrap': 'green', 'Gentoo': 'blue'}\naxes[0].scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'], c=penguins['species'].map(species_colors))\naxes[0].set_title('Actual Clusters')\naxes[0].set_xlabel('Bill Length (mm)')\naxes[0].set_ylabel('Bill Depth (mm)')\naxes[0].set_xlim(30, 60)\naxes[0].set_ylim(5, 35)\n# Create legend\nfrom matplotlib.lines import Line2D\nadelie = Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Adelie')\nchinstrap = Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Chinstrap')\ngentoo = Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Gentoo')\naxes[0].legend(handles=[adelie, chinstrap, gentoo], loc='upper right')\n\n# Plot for k-means\ncluster_colors = {0: 'red', 1: 'green', 2: 'blue'}\naxes[1].scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'], c=kmeans_pred.map(cluster_colors))\naxes[1].set_title('K-means Clusters')\naxes[1].set_xlabel('Bill Length (mm)')\naxes[1].set_ylabel('Bill Depth (mm)')\naxes[1].set_xlim(30, 60)\naxes[1].set_ylim(5, 35)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nK-means did a good job of predicting Adelie penguins, but did not do so great for the other 2 species. Since k-means wants each point to be in the cluster with the closest centroid, k-means likes clusters that are fairly circular, and has a hard time identifying clusters with different shapes. We can see that happening in this dataset. The actual clusters are long blobs, but k-means identified more circular-looking blobs.\nLet’s take a look at the inertia of the model.\n\nkmeans.inertia_\n\n2265.066596638656\n\n\nInertia is a metric that shows how close the points in the dataset are to a cluster. It is calculated by taking the sum of the squared distances between each point and its centroid. Since the distances will depend on how spread out the values in the dataset are, a single value for inertia does not mean much on its own. It is instead used to compare against inertia values for different values of \\(k\\) for the same dataset, in order to find the optimal value for \\(k\\).\nLet’s now pretend that we do not know the correct value of \\(k\\), and we want to find it. Let’s take a look at the inertia if we adjust k.\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(penguins[['bill_length_mm', 'bill_depth_mm']])\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\nplt.plot(range(1, 10), inertias, \"ro-\")\nplt.title(\"K-means Inertia for Varying $k$\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\nplt.axis([1, 9, 0, 5000])\nplt.grid()\nplt.show()\n\n\n\n\nInertia will always decrease as \\(k\\) increases. This is because adding more centroids will only decrease the distance between each point and a centroid. Inertia will decrease until \\(k\\) equals the number of points in the dataset, because then, each centroid will be on top of each data point, so the distances will all equal zero, and so will the inertia.\nSo knowing that inertia will always decrease, we want to find the point in the graph where the inertia stops decreasing rapidly. This can be done using calculus, but it can also be done by identifying th “elbow” of the curve. Here, it looks like the elbow is at 3 or 4, so we should either set \\(k\\) to 3 or 4 in our model.\n\nAnother metric for k-means clustering is silhouette score. The silhouette score shows how well-separated the clusters are. This is the formula for silhouette score:\n\\(\\frac{b - a}{max(b, a)}\\)\n\\(b\\) is the average distance to the nearest cluster, and \\(a\\) is the average distance within a cluster. The silhouette score is always between -1 and 1, so unlike inertia, it can be used on its own as a performance metric.\nLet’s see the silhouette scores for different values of k.\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_scores = [silhouette_score(penguins[['bill_length_mm', 'bill_depth_mm']], model.labels_)\n                     for model in kmeans_per_k[1:]]\n\nplt.plot(range(2, 10), silhouette_scores, \"ro-\")\nplt.title(\"K-means Silhouette Score for Varying $k$\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\nplt.axis([1.8, 9.2, 0.35, 0.7])\nplt.grid()\nplt.show()\n\n\n\n\nThe silhouette scores indicate that \\(k=2\\) would create the best clusters. This is consistent with the fact that k-means had a hard time differentiating between Chinstrap and Gentoo penguins when \\(k\\) was 3. But the next highest silhouette score is when \\(k=3\\), so if we combine our knowledge of both inertia an silhouette score, we should come to the conclusion that \\(k=3\\) would work well. If we want more accurate clustering with k-means, we could use more than two columns to predict penguin species, but that would be hard to visualize. We could also try using StandardScaler or MinMaxScaler before doing k-means to try to make the clusters more circular."
  },
  {
    "objectID": "posts/outliers/index.html",
    "href": "posts/outliers/index.html",
    "title": "Finding Outliers using Gaussian Mixture and DBSCAN",
    "section": "",
    "text": "Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n# Suppress all warnings\nwarnings.simplefilter(\"ignore\")\n\n\nPreviously, we used the penguins dataset from Seaborn to explore clustering. Let’s use the penguins dataset again to explore ways to find outliers in a dataset. The penguins dataset contains information about 3 different penguin species.\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins = penguins.dropna()\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\n\nWe removed the rows with null values from the dataset. We will only focus on bill size by using the measurements for bill length and bill depth.\nLet’s use Gaussian mixture to find outliers. Gaussian mixture is a clustering method similar to k-means. While k-means is good at identifying circular clusters, Gaussian mixture can identify a more complex shapes, including ellipses. Gaussian mixture also provides a probability for how likely each point belongs to each cluster.\n\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=3, random_state=42)\ngm.fit(X)\ngm_pred = gm.predict(X)\nprint(np.unique(gm_pred))\n\n[0 1 2]\n\n\nWe fit the Gaussian mixture model to the data for bill size, and predicted the clusters. We know that there are 3 different penguin species, so we set n_components to 3. This made the model assign points to either cluster 0, 1, or 2. Let’s plot the clusters to see how Gaussian mixture performed.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\ncolors = ['red', 'green', 'blue']\n\n# Scatter plot for each cluster\nfor cluster_num in range(gm.n_components):\n    cluster_data = penguins[gm_pred == cluster_num]\n    axes[0].scatter(cluster_data[\"bill_length_mm\"], cluster_data[\"bill_depth_mm\"], color=colors[cluster_num], label=f\"Cluster {cluster_num}\")\n\n# Plotting cluster centers\nfor i, mean in enumerate(gm.means_):\n    axes[0].scatter(mean[0], mean[1], marker='X', color=colors[i], s=200, edgecolors='black', linewidth=2)\n\naxes[0].set_title(\"Penguins Clustering with Gaussian Mixture\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\naxes[0].legend()\n\n# Plotting actual clusters\nspecies_colors = {'Adelie': 'red', 'Chinstrap': 'green', 'Gentoo': 'blue'}\naxes[1].scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'], c=penguins['species'].map(species_colors))\naxes[1].set_title(\"Actual Clusters\")\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIt looks like Gaussian mixture did a better job than k-means for this dataset. Gaussian mixture also provides probabilities, so let’s take a look at them.\n\nprint(f\"Length: {len(gm.predict_proba(X))}\")\ngm.predict_proba(X).round(2)[:10].tolist()\n\nLength: 333\n\n\n[[1.0, 0.0, 0.0],\n [0.99, 0.01, 0.0],\n [0.99, 0.01, 0.0],\n [1.0, 0.0, 0.0],\n [1.0, 0.0, 0.0],\n [1.0, 0.0, 0.0],\n [1.0, 0.0, 0.0],\n [0.96, 0.04, 0.0],\n [1.0, 0.0, 0.0],\n [1.0, 0.0, 0.0]]\n\n\nFor each data point, there are three probabilities, each one corresponding to a cluster. So the first data point has a probability of 100% that it is in cluster 0, and 0% probability that it is in clusters 1 or 2.\nGaussian mixture also gives us the log-likelihood for each data point. This value is proportional to the model’s probability density function (PDF) at that point. A higher value means the data point is fits better with the model.\n\nprint(f\"Length: {len(gm.score_samples(X))}\")\ngm.score_samples(X)[:10].tolist()\n\nLength: 333\n\n\n[-3.8184661291427817,\n -4.139050244213168,\n -3.9896460411103667,\n -4.7302522140905205,\n -5.692483578519657,\n -3.84834834433641,\n -4.375535348056427,\n -4.457162883602522,\n -7.041180611735683,\n -9.36886035974433]\n\n\nLet’s find outliers by finding the 5% of points from this dataset that fit this model the least. We will do this by finding the points with densities less than the 5th percentile.\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 5)\noutliers = X[densities &lt; density_threshold]\n\n\nX.shape\n\n(333, 2)\n\n\n\noutliers.shape\n\n(17, 2)\n\n\nThere are 17 outliers, which is \\(333 * 0.05\\).\nLet’s plot the Gaussian mixture clusters again, but this time we will also mark the outliers.\n\n# Scatter plot for each cluster\ncolors = ['red', 'green', 'blue']\n\nfor cluster_num in range(gm.n_components):\n    cluster_data = penguins[gm_pred == cluster_num]\n    plt.scatter(cluster_data[\"bill_length_mm\"], cluster_data[\"bill_depth_mm\"], color=colors[cluster_num], label=f\"Cluster {cluster_num}\")\n\n# Scatter plot for outliers\nplt.scatter(outliers[\"bill_length_mm\"], outliers[\"bill_depth_mm\"], color='black', label='Outliers')\n\n# Plotting cluster centers\nfor i, mean in enumerate(gm.means_):\n    plt.scatter(mean[0], mean[1], marker='X', color=colors[i], s=200, edgecolors='black', linewidth=2)\n\nplt.title(\"Penguin Outliers by Clustering with Gaussian Mixture\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.legend()\nplt.show()\n\n\n\n\nWe can see that the outliers are in locations with lower density, and seem to be the least likely to fit in a cluster.\nLet’s now use DBSCAN to find outliers. DBSCAN is another clustering method. DBSCAN does not take an expected number of clusters as an input. Instead, it finds clusters based on density. If there are enough points that are close enough together, DBSCAN will create a cluster. Not all points are necessarily assigned to a cluster. If a point is too far from any point that is in a cluster, the point will be marked as an outlier.\nDBSCAN has two main inputs. eps is the maximum distance a point can be from a point in a cluster to be considered in the same cluster. min_samples is the minimum number of points that can be in a cluster.\n\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=1, min_samples=5)\ndbscan.fit(X);\n\nDBSCAN assigns a predicted label to each data point. Each label represents the cluster the point belongs to. If the label is \\(-1\\), the model predicted that that point is an outlier.\n\noutliers = X[dbscan.labels_ == -1]\nprint(np.unique(dbscan.labels_))\n\n[-1  0  1]\n\n\nThis means that DBSCAN found two clusters (clusters 0 and 1), and some outliers. Let’s plot the clusters and the outliers.\n\n# Scatter plot for normal points\nplt.scatter(X[\"bill_length_mm\"], X[\"bill_depth_mm\"], c=dbscan.labels_)\n\n# Scatter plot for outliers\nplt.scatter(outliers[\"bill_length_mm\"], outliers[\"bill_depth_mm\"], color='black', label='Outliers')\n\nplt.title(\"Penguin Outliers by Clustering with DBSCAN\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.legend()\nplt.show()\n\n\n\n\nAgain, the outliers are at the edges of the clusters. There are a few differences between the outliers determined by the two models, but they look very similar for the most part."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probablility",
    "section": "",
    "text": "probability??"
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Regression using Linear Regression and Random Forest",
    "section": "",
    "text": "Regression is a type of supervised machine learning that predicts the target value of each input data point. Regression is for predicting continuous values, while classification is for predicting discrete classes. Let’s use the diabetes dataset from scikit-learn to explore regression.\n\n\nImports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nfrom sklearn.datasets import load_diabetes\ndiabetes = load_diabetes()\nprint(f\"Shape of dataset: {diabetes.data.shape}\")\npd.DataFrame(diabetes.data, columns=diabetes.feature_names).head()\n\nShape of dataset: (442, 10)\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n\n\n\n\n\n\n\nThe diabetes dataset contains measurements for 10 baseline variables from 442 diabetes patients. The label for each data point is a continuous numeric value that measures how much the diabetes progressed in one year.\nFirst, let’s do a train test split with a test size of 20%.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n\nLet’s see if we can accurately model the data using the linear regression model. Linear regression attempts find a linear relationship between the data and the labels by finding the best-fit line for the data. Since there are 10 input variables for each diabetes patient, it would be difficult to visualize the data and the best-fit line, because it would need to be in 10 dimensions. So first, let’s limit the model to only train on one input variable: age.\n\nfrom sklearn.linear_model import LinearRegression\nage = LinearRegression()\nage.fit(X_train[:, 0].reshape(-1, 1), y_train)\ny_pred = age.predict(X_test[:, 0].reshape(-1, 1))\n\n\nplt.scatter(X_train[:, 0], y_train, color='black', label='Actual data')\nplt.plot(X_test[:, 0], y_pred, color='blue', linewidth=3, label='Prediction')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Diabetes Progression\")\nplt.title(\"Linear Regression for Age\")\nplt.legend()\nplt.show()\nprint(\"Best fit line:\")\nprint(f\"y = {age.coef_[0]}x + {age.intercept_}\")\n\n\n\n\nBest fit line:\ny = 331.0721720041364x + 153.25920153319478\n\n\nIt looks like there is only a slight positive correlation between age and diabetes progression. Let’s calculate the \\(R^2\\) value for this prediction. \\(R^2\\) is a value between 0 and 1 that represents how well the dependent variable can be predicted by the independent variables. The plot showed only a slight correlation, so we should not expect the \\(R^2\\) value to be very high.\n\nfrom sklearn.metrics import r2_score\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.008289640305771506\n\n\nLet’s now try another one of the 10 independent variables: BMI.\n\nbmi = LinearRegression()\nbmi.fit(X_train[:, 2].reshape(-1, 1), y_train)\ny_pred = bmi.predict(X_test[:, 2].reshape(-1, 1))\n\n\nplt.scatter(X_train[:, 2], y_train, color='black', label='Actual data')\nplt.plot(X_test[:, 2], y_pred, color='blue', linewidth=3, label='Prediction')\nplt.xlabel(\"BMI\")\nplt.ylabel(\"Diabetes Progression\")\nplt.title(\"Linear Regression for BMI\")\nplt.legend()\nplt.show()\nprint(f\"y = {bmi.coef_[0]}x + {bmi.intercept_}\")\n\n\n\n\ny = 998.577689137559x + 152.00335421448167\n\n\nThere is a positive correlation between BMI and diabetes progression, and the correlation is stronger than in the example with age and diabetes progression. We should expect a higher \\(R^2\\) value.\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.2333503981587215\n\n\nNow, let’s use all 10 input variables in our linear regression model.\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.4526027629719196\n\n\nThe \\(R^2\\) value when using all 10 input variables is even higher than when we only used BMI. This means the other 9 input variables combined helped the model predict more accurate values. But using more input variables would not necessarily improve the model’s performance, as some input variables may not be good predictors of diabetes progression.\nLet’s now try using random forest. We used random forest when doing classification, but it can also be used for regression. Instead of having each decision tree vote to determine the final prediction, for regression, the prediction is the average of the outputs from each decision tree. Random forest is a nonlinear model, so it should be more flexible when trying to understand more complex relationships in the data.\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nr2 = r2_score(y_test, y_pred)\nprint(f\"R^2 Score: {r2}\")\n\nR^2 Score: 0.4428225673999313\n\n\nIn our example, the \\(R^2\\) value when using random forest is actually slightly worse than the \\(R^2\\) value when using linear regression! This could indicate that there is a fairly strong linear relationship in the data, and random forest is overthinking, but if that were the case, the \\(R^2\\) value for linear regression would be higher. The actual reason may be because there is no strong relationship when considering all 10 input variables while training, so both models struggle."
  }
]